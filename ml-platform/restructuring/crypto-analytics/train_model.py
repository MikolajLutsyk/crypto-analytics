import pandas as pd
import numpy as np
from catboost import CatBoostClassifier
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
import pickle
import warnings
warnings.filterwarnings('ignore')

def load_feature_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ features.csv"""
    try:
        df = pd.read_csv("data/features.csv", index_col="open_time", parse_dates=True)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –∏–∑ data/features.csv")
        return df
    except FileNotFoundError:
        print("‚ùå –§–∞–π–ª data/features.csv –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ feature_engineering.py")
        return None

def prepare_feature_data(df):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    # –£–¥–∞–ª—è–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –Ω–µ–Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    exclude_cols = ['close_future', 'future_return', 'target_direction', 'target_3class']
    
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    feature_cols = [col for col in numeric_cols if col not in exclude_cols]
    
    X = df[feature_cols].fillna(0)
    y = df['target_direction']
    
    print(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    return X, y, feature_cols

def select_best_features(X, y, k=30):
    """–í—ã–±–æ—Ä –ª—É—á—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    selector = SelectKBest(f_classif, k=min(k, X.shape[1]))
    selector.fit(X, y)
    selected_features = X.columns[selector.get_support()].tolist()
    
    print(f"üéØ –í—ã–±—Ä–∞–Ω–æ {len(selected_features)} –ª—É—á—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    return selected_features

def create_time_based_split(df, test_size=0.2):
    """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Å–ø–ª–∏—Ç–∞ (–≤–∞–∂–Ω–æ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤)"""
    split_idx = int(len(df) * (1 - test_size))
    train_mask = df.index <= df.index[split_idx]
    test_mask = df.index > df.index[split_idx]
    
    return train_mask, test_mask

def prepare_categorical_features(selected_features):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - –¢–û–õ–¨–ö–û –Ω–∞—Å—Ç–æ—è—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ"""
    cat_features_indices = []
    cat_features_names = []
    
    # –¢–û–õ–¨–ö–û –Ω–∞—Å—Ç–æ—è—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    true_categorical = ['hour', 'day_of_week', 'day_of_month', 'is_weekend']
    
    for i, col in enumerate(selected_features):
        if col in true_categorical:
            cat_features_indices.append(i)
            cat_features_names.append(col)
    
    print(f"üè∑Ô∏è –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(cat_features_indices)}): {cat_features_names}")
    return cat_features_indices

def train_model(X, y, selected_features):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ CatBoost —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∫–ª–∞—Å—Å–æ–≤"""
    # –í—Ä–µ–º–µ–Ω–Ω–æ–π —Å–ø–ª–∏—Ç
    train_mask, test_mask = create_time_based_split(X)
    
    X_train = X[selected_features].loc[train_mask]
    X_test = X[selected_features].loc[test_mask]
    y_train = y.loc[train_mask]
    y_test = y.loc[test_mask]
    
    print(f"üìà –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(X_train)}")
    print(f"üìä –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(X_test)}")
    print(f"üìä –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ:")
    print(y_train.value_counts().sort_index())
    
    # –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ê –ö–õ–ê–°–°–û–í –° –ü–û–ú–û–©–¨–Æ SMOTE
    try:
        from imblearn.over_sampling import SMOTE
        print("üîÑ –ü—Ä–∏–º–µ–Ω—è–µ–º SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤...")
        
        smote = SMOTE(random_state=42)
        X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
        
        print(f"üìà –†–∞–∑–º–µ—Ä —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(X_train_balanced)}")
        print(f"üìä –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ SMOTE:")
        print(pd.Series(y_train_balanced).value_counts().sort_index())
        
    except ImportError:
        print("‚ö†Ô∏è  imblearn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É CatBoost")
        X_train_balanced, y_train_balanced = X_train, y_train
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    cat_features_indices = prepare_categorical_features(selected_features)
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ - –ò–°–ü–û–õ–¨–ó–£–ï–ú –°–ë–ê–õ–ê–ù–°–ò–†–û–í–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï
    model = CatBoostClassifier(
        iterations=1000,
        learning_rate=0.05,
        depth=8,
        l2_leaf_reg=3,
        random_strength=0.5,
        bagging_temperature=0.8,
        od_type='Iter',
        od_wait=50,
        loss_function='Logloss',
        eval_metric='Accuracy',
        random_seed=42,
        verbose=100,
        auto_class_weights='Balanced'  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞
    )
    
    # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –°–ë–ê–õ–ê–ù–°–ò–†–û–í–ê–ù–ù–´–• –¥–∞–Ω–Ω—ã—Ö
    model.fit(
        X_train_balanced, y_train_balanced,
        eval_set=(X_test, y_test),
        cat_features=cat_features_indices,
        use_best_model=True
    )
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏
    y_pred = model.predict(X_test)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, f1_score
    
    accuracy = accuracy_score(y_test, y_pred)
    balanced_acc = balanced_accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    
    print(f"\nüéØ –¢–æ—á–Ω–æ—Å—Ç—å (Accuracy): {accuracy:.3f}")
    print(f"‚öñÔ∏è  –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {balanced_acc:.3f}")
    print(f"üìä Precision: {precision:.3f}")
    print(f"üìä Recall: {recall:.3f}")
    print(f"üìä F1-score: {f1:.3f}")
    
    print("\nüìä Classification Report:")
    print(classification_report(y_test, y_pred, target_names=["DOWN", "UP"]))
    
    # Confusion Matrix
    plt.figure(figsize=(8, 6))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
                xticklabels=["DOWN", "UP"], yticklabels=["DOWN", "UP"])
    plt.title("Confusion Matrix (–ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏)")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.tight_layout()
    plt.savefig('confusion_matrix_balanced.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Feature Importance
    feature_importance = model.get_feature_importance()
    feature_importance_df = pd.DataFrame({
        'feature': selected_features,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    plt.figure(figsize=(10, 8))
    sns.barplot(data=feature_importance_df.head(20), x='importance', y='feature')
    plt.title('Top 20 Feature Importance (–ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏)')
    plt.tight_layout()
    plt.savefig('feature_importance_balanced.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return model, accuracy, X_test, y_test, y_pred, feature_importance_df

def save_model(model, features, accuracy, feature_importance):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
    model_data = {
        "model": model,
        "features": features,
        "accuracy": accuracy,
        "feature_importance": feature_importance,
        "timestamp": pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    }
    
    with open("improved_catboost_model.pkl", "wb") as f:
        pickle.dump(model_data, f)
    
    print("üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ ‚Üí improved_catboost_model.pkl")

def run_improved_pipeline():
    """–û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è"""
    print("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—É—á–µ–Ω–∏—è...")
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = load_feature_data()
    if df is None:
        return
    
    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X, y, all_features = prepare_feature_data(df)
    
    # 3. –í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    selected_features = select_best_features(X, y, k=40)
    
    # 4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model, accuracy, X_test, y_test, y_pred, feature_importance = train_model(X, y, selected_features)
    
    # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    save_model(model, selected_features, accuracy, feature_importance)
    
    # 6. –û—Ü–µ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if accuracy >= 0.6:
        print(f"üéâ –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.2%}")
        print("‚úÖ –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–ª–∞ —Ü–µ–ª–µ–≤–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ 60%+")
    elif accuracy >= 0.55:
        print(f"‚ö†Ô∏è  –ü—Ä–∏–µ–º–ª–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {accuracy:.2%}")
        print("‚ÑπÔ∏è  –ú–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —É–ª—É—á—à–∏—Ç—å —á–µ—Ä–µ–∑ –Ω–∞—Å—Ç—Ä–æ–π–∫—É –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    else:
        print(f"‚ùå –ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.2%}")
        print("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
    
    # –í—ã–≤–æ–¥ —Ç–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\nüèÜ –¢–æ–ø-10 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    for i, row in feature_importance.head(10).iterrows():
        print(f"  {i+1:2d}. {row['feature']}: {row['importance']:.4f}")

if __name__ == "__main__":
    run_improved_pipeline()